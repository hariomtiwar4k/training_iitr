# -*- coding: utf-8 -*-
"""Day 9 - Simple Neural Network From Scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pt5THrT_Aaf-NA5gNWh13HNIr2eGh_td
"""

import numpy as np

input = np.array([[0,1,0],[0,0,1],[1,1,1],[1,0,1]])
output = np.array([[0],[0],[1],[1]])

input

output

np.random.seed(10)
weights = np.random.random((3,1))
weights

sum = np.dot(input,weights) + 0.02
sum

### Create the activation function (sigmoid)
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

sum = np.dot(input,weights) + 0.02
pred_output = sigmoid(sum)
error = output - pred_output
error

def gradient(x):
  return x * (1-x)

for i in range(1500):
  sum = np.dot(input,weights) + 0.02
  pred_output = sigmoid(sum)
  error = output - pred_output
  ## Apply the gradient Descent
  adjustment = error * gradient(pred_output)
  ##Apply the backpropogation
  weights += np.dot(input.T,adjustment) 
error

pred_output.round()

input.shape

adjustment.shape

